# 基础知识

## 条件概率

条件概率是指事件A在另一个事件B已经发生条件下发生的概率。条件概率表示为P(A/B)，读作“在B条件下A的概率”。
 若只有两个事件A，B：

![p(A|B)=\frac{P(AB)}{P(B)}](https://math.jianshu.com/math?formula=p(A%7CB)%3D%5Cfrac%7BP(AB)%7D%7BP(B)%7D)

## 全概率公式

表示若事件A1，A2，…，An构成一个完备事件组且都有正概率，则对任意一个事件B都有公式成立：
![P(B)=P(AB+\bar{A}B) =P(AB)+P(\bar{A}B)=P(A)P(B|A)+P(\bar{A})P(B|\bar{A})](https://math.jianshu.com/math?formula=P(B)%3DP(AB%2B%5Cbar%7BA%7DB)%20%3DP(AB)%2BP(%5Cbar%7BA%7DB)%3DP(A)P(B%7CA)%2BP(%5Cbar%7BA%7D)P(B%7C%5Cbar%7BA%7D))同时P(B)也可以表示为：
![P(B)=P(A_{1}B)+P(A_{2}B)+...+P(A_{n}B)](https://math.jianshu.com/math?formula=P(B)%3DP(A_%7B1%7DB)%2BP(A_%7B2%7DB)%2B...%2BP(A_%7Bn%7DB))
![=\sum P(A_{i}B)](https://math.jianshu.com/math?formula=%3D%5Csum%20P(A_%7Bi%7DB))
![=\sum P(B|A_{i})*P(A_{i})](https://math.jianshu.com/math?formula=%3D%5Csum%20P(B%7CA_%7Bi%7D)*P(A_%7Bi%7D))

## 贝叶斯公式

由条件概率公式可以得到**贝叶斯表达式**：
$$
p(A|B)=\frac{P(AB)}{P(B)}=\frac{P(B|A)*P(A)}{P(B)}
$$
将全概率公式代入到条件概率公式当中，对于事件![A_{k}](https://math.jianshu.com/math?formula=A_%7Bk%7D)和事件B有：

![P(A_{k}|B)=\frac{P(B|A_{k})*P(A_{k})}{\sum P(B|A_{i})*P(A_{i})}](https://math.jianshu.com/math?formula=P(A_%7Bk%7D%7CB)%3D%5Cfrac%7BP(B%7CA_%7Bk%7D)*P(A_%7Bk%7D)%7D%7B%5Csum%20P(B%7CA_%7Bi%7D)*P(A_%7Bi%7D)%7D)

P(A)是 A 的**先验概率**，之所以称为“先验”是因为它不考虑任何 B 方面的因素。

P(A|B)是已知 B 发生后 A 的条件概率，也由于得自 B 的取值而被称作 A 的后验概率。

P(B|A)是已知 A 发生后 B 的条件概率，也由于得自 A 的取值而被称作 B 的后验概率。

P(B)是 B 的先验概率，也作标淮化常量（normalizing constant）。

## 朴素贝叶斯分类器的公式

假设某个体有n项特征(Feature)，分别为F1，F2，...，Fn。现在有m个类别（Category），分别为C1，C2，C3，...，Cm。贝叶斯分类器就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：
 ![P(C|F_{1}F_{2}...F_{n}) = \frac{P(F_{1}F_{2}...F_{n}|C)P(C)}{P(F_{1}F_{2}...F_{n})}](https://math.jianshu.com/math?formula=P(C%7CF_%7B1%7DF_%7B2%7D...F_%7Bn%7D)%20%3D%20%5Cfrac%7BP(F_%7B1%7DF_%7B2%7D...F_%7Bn%7D%7CC)P(C)%7D%7BP(F_%7B1%7DF_%7B2%7D...F_%7Bn%7D)%7D)
 由于**P(F1F2...Fn)**对于所有的分类都是相同的，可以省略，问题就变成了求![P(F_{1}F_{2}...F_{n}|C)P(C)](https://math.jianshu.com/math?formula=P(F_%7B1%7DF_%7B2%7D...F_%7Bn%7D%7CC)P(C))
 的最大值。
 朴素贝叶斯分类器则是更进一步，**假设所有特征都彼此独立**，因此：
 ![P(F_{1}F_{2}...F_{n}|C)P(C) = P(F_{1}|C)P(F_{2}|C)...P(F_{n}|C)P(C)](https://math.jianshu.com/math?formula=P(F_%7B1%7DF_%7B2%7D...F_%7Bn%7D%7CC)P(C)%20%3D%20P(F_%7B1%7D%7CC)P(F_%7B2%7D%7CC)...P(F_%7Bn%7D%7CC)P(C))
 上式等号右边的每一项，都可以从统计资料中得到，由此就可以计算出每个类别对应的概率，从而找出最大概率的那个类。虽然"所有特征彼此独立"这个假设，在现实中不太可能成立，但是它可以大大简化计算。

## 拉普拉斯平滑(Laplace smoothing)

也就是参数为1时的贝叶斯估计，当某个分量在总样本某个分类中（观察样本库/训练集）从没出现过，会导致整个实例的计算结果为0。为了解决这个问题，使用拉普拉斯平滑进行处理。它的思想非常简单，就是对先验概率的分子（划分的计数）加1，分母加上类别数；对条件概率分子加1，分母加上对应特征的可能取值数量。这样在解决零概率问题的同时，也保证了概率和依然为1.

这里举个例子，假设在文本分类中，有3个类，C1,C2,C3,在指定的训练样本中，某个词语F1，在各个类中观测计数分别为0，990，10，则概率为P(F1|C1)=0,P(F1|C2)=0.99,P(F1|C3)=0.01,对这三个量使用拉普拉斯平滑的计算方法如下：1/1003=0.001，991/1003=0.988，11/1003=0.011

