## Logistic Regression

![【机器学习】逻辑回归（非常详细）](笔记.assets/v2-35393b75f51c81bb3c09774e76a7d91c_1440w.jpg)

虽然被称为回归，但其实际上是分类模型，常用于二分类。

Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。

### 假设函数

Sigmoid函数，也称为逻辑函数（Logistic function）：

![[公式]](https://www.zhihu.com/equation?tex=g%28z%29%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)

![img](笔记.assets/v2-1562a80cf766ecfe77155fa84931e745_720w.png)

逻辑回归的假设函数形式如下：

![[公式]](https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+g%28%5Ctheta%5ET+x%29%2C+g%28z%29%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D)

所以：

![[公式]](https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETx%7D%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=x) 是我们的输入， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 为我们要求取的参数。

### 决策边界

在逻辑回归中，我们预测：

当ℎ𝜃(𝑥) >= 0.5时，预测 𝑦 = 1。 

当ℎ𝜃(𝑥) < 0.5时，预测 𝑦 = 0 。

根据上面绘制出的 **S** 形函数图像，我们知道当

𝑧 = 0 时 𝑔(𝑧) = 0.5 

𝑧 > 0 时 𝑔(𝑧) > 0.5 

𝑧 < 0 时 𝑔(𝑧) < 0.5 

又 𝑧 = 𝜃𝑇𝑥 ，即：

𝜃𝑇𝑥 >= 0 时，预测 𝑦 = 1 

𝜃𝑇𝑥 < 0 时，预测 𝑦 = 0

（待修改

*用极大释然估计来估计$\theta$*

*极大似然估计*

*极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。*）

### 代价函数

逻辑回归中的代价函数 ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29) :

![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+-%5Cfrac%7B+1+%7D%7B+m+%7D%5B%5Csum_%7B+i%3D1+%7D%5E%7B+m+%7D+%28%7By%5E%7B%28i%29%7D+%5Clog+h_%5Ctheta%28x%5E%7B%28i%29%7D%29+%2B+%281-y%5E%7B%28i%29%7D%29+%5Clog+%281-h_%5Ctheta%28x%5E%7B%28i%29%7D%29%7D%29%5D)

代价函数𝐽(𝜃)是一个凸函数，并且没有局部最优值。

**梯度下降算法**

**Repeat** { 
$$
\theta _j=\theta _j-\frac{\partial J(\theta )}{\partial \theta _j}
$$
(**simultaneously update all** 𝜃𝑗 ) 

}

### 多类别分类

多个二分类，取最高概率的类别作为结果

### 正则化

![image-20211008155425129](笔记.assets/image-20211008155425129.png)

